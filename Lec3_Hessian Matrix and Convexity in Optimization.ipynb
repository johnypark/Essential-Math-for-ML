{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7/5/2021 John Park\n",
    "\n",
    "References:\n",
    "http://people.seas.harvard.edu/~yaron/AM221/lecture_notes/AM221_lecture10.pdf\n",
    "\n",
    "Many machine learning problems comes down to optimization, and convex optimization. The success of these convex optimization solutions rely on setting up a cost function in a parameter space as a convex function.\n",
    "\n",
    "In Mathematics, convexity is one of the most important properties that lecturers heavily emphazises on. So did the professors in my undergraduate institutions, in Numerical analysis, linear albegra, calculus, real analysis. However, they rarely tell you why convexity is important or what other real world implications are there that are using this properties for problem solving. The problem of mathematics major is that their focus is on abstract materials so ordinary student can never find the connection of those enriched legacies to modern day problems. Such a shame, but I was one of the ordinary students, so long after graduating with mathematics degree I learned how to connect with these legacies.\n",
    "\n",
    "### Section 1. Theories\n",
    "\n",
    "###### Theorem / Lemma\n",
    "\n",
    "1. Let $f:R^{n}â†’R$ and $S$ be a convex subset of $R^{n}$. $\\forall$ $x$, $y \\in S$: \n",
    "\n",
    "    $f$ is convex iff $ f(y) \\geq f(x) + \\nabla f(x)^{T}  (y-x)$.\n",
    "\n",
    "\n",
    "\n",
    "2. If Hessian matrix of cost function $L$ is positive semi definite then $L$ is convex.\n",
    " - Proof is taylor expansion, if hessian is positive semi definite then convex definition satifies.\n",
    " $ f(y) = f(x) + \\nabla f(x)^{T}(y-x)+ 1/2 (y-x)^{T} H_{f}(z)(y-x) $. If $ H_{f}(z) $ is positive semi definite, then $ (y-x)^{T} H_{f}(z)(y-x) \\geq 0$. Therfore, $f(y) \\geq f(x) + \\nabla f(x)^{T}(y-x)$, which is definition of convexity.\n",
    " \n",
    " \n",
    "3. If $S$ is open convex set and $L$ is convex, then Hessian of $L$ is positive semi definite. \n",
    "\n",
    "\n",
    "4. In a convex set, with convex function $L$, if $\\bar x$ is stationary point, then $\\bar x$ is global minimum. \n",
    " - Proof is quite straightforward as you have stationary point that leaves the first derivative part zero, therefore $f(y) \\geq f(\\bar x) $, for all $y \\in S$.\n",
    " \n",
    " \n",
    "\n",
    "\n",
    "###### Steepst Ascent Algorithm \n",
    "\n",
    "\n",
    "In order to minmize a convex function, your aim is to find a stationary point. In convex function, if you follow the direction that the gradient is highest, you will get to the stationary point eventually. \n",
    "\n",
    " - Given $x^{k}$, $ \\alpha _{k}= \\argmin_{\\alpha \\geq 0} (x^{k} - \\alpha \\nabla f(x^{k})) $.\n",
    "\n",
    "\n",
    "\n",
    "###### Learning rate\n",
    "\n",
    "In reality, you need to define the increment that you are differentiating the cost function in the parameter space. this is called the learning rate, eta ($ \\eta $). Is there another \n",
    "\n",
    "\n",
    "###### Lagrange multiplier and KKT condition\n",
    "\n",
    "\n",
    "\n",
    "###### Questions and Answers \n",
    "Q1. How do you evaluate if a matrix is positive semi definite? \n",
    "A1. If its eigenvalues are all non-negative. If its eigenvalues are all positive, then it is positive definite. \n",
    "\n",
    "Q2. What is the geometrical meaning of positive semi definiteness? \n",
    "A2. The reponse variable has bottom\n",
    "\n",
    "Q3. What is a good way to find a global minima when there are many local minima?\n",
    "A3. Stochastic gradient descent -> and what else?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 2. Applicaiton\n",
    "\n",
    "###### Example - between squared error or cross entropy for the cost function in logistic regression \n",
    "https://stats.stackexchange.com/questions/326350/what-is-happening-here-when-i-use-squared-loss-in-logistic-regression-setting\n",
    "\n",
    "Take home: cross entropy loss makes the hessian positive semi definite, whereas squared error does not. Convexity is not guarenteed therefore our parameter values do not guarantee global mimimum, there are some chances that squared error loss will lead us to parameters from local minima. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
