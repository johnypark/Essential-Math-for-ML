{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear regression is a great place to start with as an example problem to understand the essentials of machine learning. \n",
    "\n",
    "It is a simpliest setting to solve real-world problem when solution for $Ax=b$ does not exist.\n",
    "\n",
    "\n",
    "\n",
    "Let's go back to the basic, elementary school algebra.\n",
    "\n",
    "We care about the solution for simple equaiton for linear system of two variables, two equations with two unknowns:\n",
    "\n",
    "$$ 3x-y=7$$\n",
    "$$ 2x+3y=1 $$\n",
    "\n",
    "Rewriting it in matrix algebra:\n",
    "\n",
    "$$ \\begin{bmatrix} 3 & -1 \\\\ 2  & 3 \\end{bmatrix}  \\begin{bmatrix} x \\\\ y \\end{bmatrix} =\t\\begin{bmatrix} 7 \\\\ 1\\end{bmatrix} $$\n",
    "\n",
    "Is a problem of solving $ Ax=b $ where $A=\\begin{bmatrix} 3 & -1 \\\\ 2  & 3 \\end{bmatrix}$ and $b=\\begin{bmatrix} 7 \\\\ 1\\end{bmatrix}$.\n",
    "\n",
    "Is this case, determinant of $A$ exists as $A^{-1}$ = \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           [,1]       [,2]\n",
      "[1,]  0.2727273 0.09090909\n",
      "[2,] -0.1818182 0.27272727\n"
     ]
    }
   ],
   "source": [
    "A_inv=solve(matrix(c(3,2,-1,3),nrow = 2))\n",
    "print(A_inv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And thus $x=A^{-1}b=\\begin{bmatrix} 2\\\\-1\\end{bmatrix}$, as"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     [,1]\n",
      "[1,]    2\n",
      "[2,]   -1\n"
     ]
    }
   ],
   "source": [
    "b=A_inv%*%matrix(c(7,1))\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we think of a situation where the system is not solvable, such as there are more eqautions than unknowns (or $det(A)=0$ case!). \n",
    "\n",
    "Example from pp 162 (Strang, 2006).\n",
    "\n",
    "$$ 1x_{1}+ 2x_{2}=b_{1} $$\n",
    "$$ 1x_{1}+3x_{2}=b_{2} $$\n",
    "$$ 0x_{1}+0x_{2}=b_{3} $$\n",
    "\n",
    "There is no direct solution to this problem. And this is what any other real-world problems mostly look like. \n",
    "Column space of $A$ is a two-dimensional plane within the three-dimensional space. \n",
    "\n",
    "Taking partial derivative to minimize equared error term gives \n",
    "\n",
    "$$ 2A^{T}Ax-2A^{T}b=0 $$ \n",
    "\n",
    "Which is the same as finding vector perpendicaular to the column space, error vector being $ e =b-A\\hat{x}$\n",
    "Tus $ A^{T}(b-A\\hat{x})=0 $\n",
    "The projection matrics is \n",
    "$$ P=A(A^{T}A)^{-1}A^{T}$$\n",
    "\n",
    "<img src=\"Plot1_210621_164334.jpg\" width=300 height=300 />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In statistics, we think of a situation where $b=Ax+\\varepsilon $ where x are $\\beta $ which is regression coefficient. Re-writing in regression term: $$y=X\\beta+\\varepsilon$$. $$\\varepsilon \\sim N(0,\\sigma)$$\n",
    "Note that $x$ in linear algebra became $\\beta$ and matrix $A$ became $X$. \n",
    "Now, since $$y-X\\beta \\sim N(0,\\sigma)$$\n",
    "We can express the likelihood of our data given the wieghts $\\beta$, thus writing it as function of $\\beta$\n",
    "\n",
    "$$P(X,y|\\beta) \\propto L(\\beta)$$\n",
    "Where the pdf of $\\varepsilon$ is \n",
    "$$ \\propto Exp[-1/2(y-X\\beta)^{T}1/\\sigma I (y-X\\beta)] $$\n",
    "Therefore, minima of log likelihood is\n",
    "$$ \\frac{\\partial}{\\partial \\beta}log L(\\beta) \\propto (y-X\\beta)^{T}(y-X\\beta)=0 $$\n",
    "$$ \\frac{\\partial}{\\partial \\beta} (y-X\\beta)^{T}(y-X\\beta) $$\n",
    "Through product rule\n",
    "$$ = -(y-X\\beta)^{T}X-(y-X\\beta)^{T}X$$\n",
    "$$ = -2 (y-X\\beta)^{T}X $$\n",
    "$$ = -2 (y^{T}-\\beta^{T}X^{T})X $$\n",
    "$$ \\propto (y^{T}-\\beta^{T}X^{T})X =0. $$\n",
    "Therefore, \n",
    "$$\\beta^{T}X^{T}X=y^{T}X $$\n",
    "$$ \\beta^{T}=y^{T}X(X^{T}X)^{-1} $$\n",
    "Then,\n",
    "$$ \\beta=(y^{T}X(X^{T}X)^{-1})^{T} $$\n",
    "$$ \\beta=(X(X^{T}X)^{-1})^{T}y $$\n",
    "$$ \\beta=((X^{T}X)^{-1})^{T}X^{T}y $$\n",
    "Since $(X^{T}X)^{-1}={(X^{T}X)^{-1}}^{T} $,\n",
    "$$ \\beta=(X^{T}X)^{-1}X^{T}y. $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IS THIS SIMILAR?\n",
    "\n",
    "$ \\beta=(X^{T}X)^{-1}X^{T}y. $ in statistics is actually the same as $ x=(A^{T}A)^{-1}A^{T}b$ in linear algebra. \n",
    "\n",
    "\n",
    "How did this happen?\n",
    "\n",
    "\n",
    "We used squared error term for cost funciton (loss function). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.0.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
