{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference\n",
    "- After Friedman\n",
    "- The elements of Statistical Learning (ESL)\n",
    "- Trevor Hastie - Gradient Boosting Machine Learning, Dec 2, 2014. \n",
    "https://www.youtube.com/watch?v=wPqtzj5VZus\n",
    "\n",
    "Keywords\n",
    "\n",
    "- Bootstrap (ss8.4, ESL)\n",
    "        - Take a random sample of training data, without duplication, and do that for many times. \n",
    "        - way of asesing accuracy (uncertainty) of a paramtere estimate or prediciton\n",
    "        - can used to improve the estimate or prediction itself. \n",
    "        - Bootstrap mean is approximate of posterior average. \n",
    "- Bagging: Bootstrap aggregation.\n",
    "        - Take a random sample of training data, with duplicate. (with replacement). If we have N training data, take n samples for each bag, for B number of bags. Shakes the data up. Grow thousands of trees. \n",
    "        - Can take the variance down. \n",
    "        - average the prediction over a collection of bootstrap samples. -> reduce variance!\n",
    "        - Averaging method; at given terminal node, probability outcome will be associated with any given data point. You average the outcomes. Each tree will give you the probability at this particular point that you want to make the prediction. Average the probabilities. \n",
    "        -Effects: Smoothes the decision boundaries. \n",
    "        -Downside: if the bagged trees are correlated, it limits the ability to reduce the variance. So the key is to have the trees uncorrelated from each other. \n",
    "- Random Forests ss15, ESL, pp 587.\n",
    "Refinement of Bagged trees, a way of decorrelating these trees some more. Additional randomness when growing the tree.\n",
    "        - Each time you grow the tree; pick random features among full features. Typically sqrt(p), where p: number of features. \"Randomness for which variable is used for splitting (Hastie, 2014).\" -> decorrelate trees\n",
    "        - Result in much more reduction in variance when doing the avaraging. \n",
    "        - You get leave-one-out prediciton error for free, by getting error rate of samples that where not involved in tree-generation at each bagging stage. \n",
    "        - You make quite busy trees, therefore bias is very small, but high variance, and you reduce the variance by averaing the trees. \n",
    "    - $ Var\\hat f_{rf}(x)= \\rho (x) \\sigma^{2}(x) $; sampling correlation reduces by the function of number of features selected at each node for tree generation. \n",
    "    \n",
    "    \n",
    "- Boosting; another way of averging trees, learn from the previous; Boosting dominats random forest. Chap 10. \n",
    "    - Adaboost (1997) first proposed by Fraud and Shapiro\n",
    "    - Boosting reweight the trees\n",
    "    - Grow trees to fix up the mistakes\n",
    "    - Not growing i.i.d. trees.\n",
    "    - boosting depending on the problem you might grow very shallow trees. \n",
    "    - boosting with stumps. Stumps is a tree with single split. \n",
    "    - Boosting does not use training error to learn: because test error continues to go down after training error hits zero. It must be using something else. \n",
    "    - Boosting does overfit. \n",
    "    - Boosting is building special kind of model: \"Stagewise additive modeling.\" It builds an additive model that each trees are basis functions. (weak learners).\n",
    "    - However not in normal sense, because in general cases in additive model, we optimize parameters jointly with gradient-type of convex optimziation methods. \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.0.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
