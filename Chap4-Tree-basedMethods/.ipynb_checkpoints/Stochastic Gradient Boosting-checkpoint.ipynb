{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference\n",
    "- After Friedman\n",
    "- The elements of Statistical Learning (ESL)\n",
    "- Trevor Hastie - Gradient Boosting Machine Learning, Dec 2, 2014. \n",
    "https://www.youtube.com/watch?v=wPqtzj5VZus\n",
    "\n",
    "Keywords\n",
    "\n",
    "- Bootstrap (ss8.4, ESL)\n",
    "        - Take a random sample of training data, without duplication, and do that for many times. \n",
    "        - way of asesing accuracy (uncertainty) of a paramtere estimate or prediciton\n",
    "        - can used to improve the estimate or prediction itself. \n",
    "        - Bootstrap mean is approximate of posterior average. \n",
    "- Bagging: Bootstrap aggregation.\n",
    "        - Take a random sample of training data, with duplicate. (with replacement). If we have N training data, take n samples for each bag, for B number of bags. Shakes the data up. Grow thousands of trees. \n",
    "        - Can take the variance down. \n",
    "        - average the prediction over a collection of bootstrap samples. -> reduce variance!\n",
    "        - Averaging method; at given terminal node, probability outcome will be associated with any given data point. You average the outcomes. Each tree will give you the probability at this particular point that you want to make the prediction. Average the probabilities. \n",
    "        -Effects: Smoothes the decision boundaries. \n",
    "        -Downside: if the bagged trees are correlated, it limits the ability to reduce the variance. So the key is to have the trees uncorrelated from each other. \n",
    "- Random Forests; avering indepedent: Refinement of Bagged trees, a way of decorrelating these trees some more. Additional randomness when growing the tree.\n",
    "        - Each time you grow the tree; pick random features among full features. Typically sqrt(p), where p: number of features. \"Randomness for which variable is used for splitting (Hastie, 2014).\"\n",
    "- Boosting; way of averging trees, learn from the previous; Boosting dominats random forest\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.0.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
